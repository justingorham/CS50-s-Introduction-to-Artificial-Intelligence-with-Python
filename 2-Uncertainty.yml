Current Place: https://youtu.be/uQmYZTTqDC0?t=4169
Probability:
Possible Worlds:
  - represented by ω
  - each world has probability represented by P(ω)
  - 0 ≤ P(ω) ≤ 1
  - Σ_{ω∈Ω} P(ω) = 1:
      - The sum of the probability of all possible worlds equals 1
Unconditional Probability:
  - degree of belief in a proposition in the absence of any other evidence
Conditional Probability:
  - degree of belief in a proposition given some evidence that has already been revealed
  - P(a|b)
  - The probability of a given
  - Formula:
      - P(a|b) = P(a ∧ b)
      - -----------
      - P(b)
  - The fraction of all of the worlds where a and b is true over all of the worlds where b is true
  - Formula often expressed as:
      - P(a ∧ b) = P(b)P(a|b)
      - P(a ∧ b) = P(a)P(b|a)
random variable:
  - a variable in probability theory with a domain of possible values it can take on
probability distribution:
  - the probability of each value for a random variable
independence:
  - the knowledge that one event occurs does not affect the probability of the other event
  - for independent variables P(a ∧ b) = P(a)P(b|a) = P(a)P(b)
Bayes' Rule:
  - Formula:
      - P(b|a) = P(b)P(a|b)
      - ----------
      - P(a)
  - Used in the following manner:
      - Knowing P(visible effect | unknown cause)
      - we can calculate P(unknown cause | visible effect)
Joint Probability:
  - the likelihood of multiple different events simultaneously
  - the probability distribution of each of the possible combinations of values that random variables can take on
  - can infer conditional probability from joint probability
  - conditional probability is proportional to the joint probability:
      - P(a | b) = αP(a,b)
      - The comma is interchangeable with ∧
Probability rules:
  - Negation:
      - P(¬a) = 1 - P(a)
  - Inclusion-Exclusion:
      - P(a ∨ b) = P(a) + P(b) - P(a ∧ b)
  - Marginalization:
      - P(a) = P(a,b) + P(a, ¬b)
      - P(X = x_i) = ∑_j P(X = x_i, Y = y_j)
  - Conditioning:
    - P(a) = P(a|b)P(b) + P(a|¬b)P(¬b)
    - P(X = x_i) =  ∑_j P(X = x_i | Y = y_j)P(Y = y_j)
Bayesian network:
  - data structure that represents the dependencies among random variables
  - directed graph
  - each node represents a random variable
  - arrow from X to Y means X is a parent of Y
  - each node x has probability distribution P(X | Parents(X))
Inference:
  - "Query X: variable for which to compute distribution"
  - "Evidence variables E: observed variables for event e"
  - "Hidden variables Y: non-evidence, non-query variable"
  - "Goal: Calculate P(X|e)"