Current Place: https://youtu.be/uQmYZTTqDC0?t=4169
Probability:
Possible Worlds:
  - represented by ω
  - each world has probability represented by P(ω)
  - 0 ≤ P(ω) ≤ 1
  - Σ_{ω∈Ω} P(ω) = 1:
      - The sum of the probability of all possible worlds equals 1
Unconditional Probability:
  - degree of belief in a proposition in the absence of any other evidence
Conditional Probability:
  - degree of belief in a proposition given some evidence that has already been revealed
  - P(a|b)
  - The probability of a given
  - Formula:
      - P(a|b) = P(a ∧ b)
      - -----------
      - P(b)
  - The fraction of all of the worlds where a and b is true over all of the worlds where b is true
  - Formula often expressed as:
      - P(a ∧ b) = P(b)P(a|b)
      - P(a ∧ b) = P(a)P(b|a)
random variable:
  - a variable in probability theory with a domain of possible values it can take on
probability distribution:
  - the probability of each value for a random variable
independence:
  - the knowledge that one event occurs does not affect the probability of the other event
  - for independent variables P(a ∧ b) = P(a)P(b|a) = P(a)P(b)
Bayes' Rule:
  - Formula:
      - P(b|a) = P(b)P(a|b)
      - ----------
      - P(a)
  - Used in the following manner:
      - Knowing P(visible effect | unknown cause)
      - we can calculate P(unknown cause | visible effect)
Joint Probability:
  - the likelihood of multiple different events simultaneously
  - the probability distribution of each of the possible combinations of values that random variables can take on
  - can infer conditional probability from joint probability
  - conditional probability is proportional to the joint probability:
      - P(a | b) = αP(a,b)
      - The comma is somewhat interchangeable with ∧. Only difference is , is used for joint probability while ∧ for conditional
Probability rules:
  - Negation:
      - P(¬a) = 1 - P(a)
  - Inclusion-Exclusion:
      - P(a ∨ b) = P(a) + P(b) - P(a ∧ b)
  - Marginalization:
      - P(a) = P(a,b) + P(a, ¬b)
      - P(X = x_i) = ∑_j P(X = x_i, Y = y_j)
  - Conditioning:
    - P(a) = P(a|b)P(b) + P(a|¬b)P(¬b)
    - P(X = x_i) =  ∑_j P(X = x_i | Y = y_j)P(Y = y_j)
Bayesian network:
  - data structure that represents the dependencies among random variables
  - directed graph
  - each node represents a random variable
  - arrow from X to Y means X is a parent of Y
  - each node x has probability distribution P(X | Parents(X))
Inference:
  - "Query X: variable for which to compute distribution"
  - "Evidence variables E: observed variables for event e"
  - "Hidden variables Y: non-evidence, non-query variable"
  - "Goal: Calculate P(X|e)"
Inference by Enumeration:
  - P(X|e) = αP(X,e) = α∑_{y} P(X,e,y)
  - X is the query variable
  - e is the evidence
  - y ranges over values of hidden variables
  - α normalizes the result 
Approximate Inference:
Sampling:
  - choose samples in proportion to probability. More samples = better accuracy 
  - given a very unlikely event, you'll end up throwing away a lot of samples
  - Likelihood Weighting:
    - Start by fixing the values for evidence variables.
    - Sample the non-evidence variables using conditional probabilities in the Bayesian network
    - "Weight each sample by its likelihood: the probability of all of the evidence"
Uncertainty over time:
  - X_t: value at of variable at time t
Markov assumption:
  - the assumption that the current state depends on only a finite fixed number of previous states
  - Markov model
Sensor Model:
  - We don't have access to hidden/true state, but we can make observations that are based on hidden state
  - Examples:
    - Hidden State | Observation
    - robot's position | robots sensor data
    - words spoken (to Alexa or something) | audio waveforms
    - user engagement | website or app analytics
    - weather | umbrella 
Hidden Markov Model:
  - a Markov model for a system with hidden states that generate some observed event
sensor Markov assumption:
  - the assumption that the evidence variable depends only on the corresponding state
  - Tasks:
    - Task | Definition
    - filtering | given observations frm start until now, calculate distribution for current state
    - prediction | given observations from start until now, calculate distribution for a future state
    - smoothing | given observations from start until now, calculate distribution for past state
    - most likely explanation | given observations from start until now, calculate most likely sequence of states:
      - common for voice recognition


    

