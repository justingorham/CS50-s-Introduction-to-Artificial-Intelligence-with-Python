Current Place: https://youtu.be/E4M_IQG0d9g?t=3943
Supervised Learning: given a data set of input-output pairs, learn a function to map inputs to outputs
classification: supervised learning task of learning a function mapping an input point to a discrete category
example:
  - given data where we have humidity and pressure for each day, if we label each pinot as "rain" or "no rain", the applied label makes this supervised
  - f(humidity,pressure) to give us "Rain" or "No Rain".
  - we want to approximate f, so we come up with a hypothesis function h(humidity, pressure)
  - we could plot the points in a 2 axis graph (n-axis if we have n variables)
  - given our data, if given a new point, we want to predict classification
nearest-neighbor classification: algorithm that, given an input, chooses the class of the nearest data point to that input
k-nearest neighbor classification:
  - algorithm that, given an input, chooses the most common class out of the k nearest data points to that input
  - can be slow in finding neighbors
we could try to find a line that separate categories:
  - data isn't always that clean however.
back to example:
  - x_{1} = humidity
  - x_{2} = pressure
  - h(x_1, x_2) =:
      - Rain if w_0 + (w_1)(x_1) + (w_1)(x_1) ≥ 0
      - No Rain otherwise
  - to keep things in math terms, Rain = 1 and No Rain =
  - Weight Vector w: (w_0, w_1, w_2)
  - Input vector x: (1, x_1, x_2)
  - a data point is now the dot product of Weight Vector and Input Vector
  - h_{w}(x) =:
      - 1 if w · x ≥ 0
      - 0 otherwise
perceptron learning rule:
  - Given data point (x,y), update each weight according to:
      - w_{i} = w_{i} + α(y - h_{w}(x)) × x_{i}
      - w_{i} = w_{i} + α(actual value - estimate) × x_{i}
hard threshold:
  - definitely puts points into a category
  - no nuance or concept of how sure we are about the category
  - line
  - 0 or 1
soft threshold:
  - curve
  - adds nuance
  - any value between 0 and 1 inclusive
Support Vector Machines:
  - a "machine" designed to find the maximum margin separator
  - We want this since a lot of lines, decision boundaries that can be drawn
maximum margin separator:
  - boundary that maximizes the distance between any of the data points
regression:
  - supervised learning task of learning a function mapping an input point to a continuous value
Evaluating Hypotheses:
  - can think of this as an optimization problem where we want to minimize a loss function
loss function:
  - function that expresses how poorly our hypothesis performs
0-1 loss function:
  - L(actual,predicted) =:
      - 0 if actual = predicted,
      - 1 otherwise
L_{1} loss function:
  - L(actual, predicted) = |actual - predicted|
L_{2} loss function:
  - L(actual, predicted) = (actual-predicted)^{2}
overfitting:
  - a model that fits too closely to a particular data set and therefore may fail to generalize to future data
  - this is a risk if the only thing we care about is minimizing the loss function
regularization:
  - penalizing hypotheses that are ore complex to favor simpler, more general hypotheses
  - cost(h) = loss(h) + λ × complexity(h)
can also run experiments by seeing if we're overfitting:
holdout cross-validation:
  - splitting data into a training set and a test set, such that learning happens on the training set and is evaluated on the test set
k-fold cross validation:
  - splitting data into k sets, and experimenting k times, using each set as a test set once, and using remaining data a training set
scikit-learn:
 - python library